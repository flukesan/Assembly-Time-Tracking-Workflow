# Phase 4B: RAG + DeepSeek-R1 Integration

## ğŸ“‹ Overview

Phase 4B extends the Assembly Time-Tracking System with AI-powered insights using **Retrieval-Augmented Generation (RAG)** and **DeepSeek-R1** local LLM via Ollama.

### Key Features

- ğŸ§  **Natural Language Queries** - Ask questions in Thai or English
- ğŸ“Š **Automated Insights** - Daily productivity analysis
- ğŸ” **Anomaly Detection** - Statistical pattern detection
- ğŸ¯ **Smart Recommendations** - AI-powered actionable advice
- ğŸ“„ **Automated Reports** - Daily, weekly, and worker-specific reports
- ğŸ—„ï¸ **Knowledge Base** - Qdrant vector database with semantic search
- ğŸŒ **Bilingual Support** - Full Thai + English interface

---

## ğŸ—ï¸ Architecture

### Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FastAPI Application                       â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Worker Mgmt   â”‚  â”‚ Time Track   â”‚  â”‚ AI Query API    â”‚  â”‚
â”‚  â”‚ (Face+Badge)  â”‚  â”‚ (Productivity)â”‚  â”‚ (NL Queries)    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                    â”‚                     â”‚
         â–¼                    â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL   â”‚    â”‚   Qdrant     â”‚    â”‚     Ollama       â”‚
â”‚ (TimescaleDB)â”‚    â”‚ (Vector DB)  â”‚    â”‚ (DeepSeek-R1)    â”‚
â”‚              â”‚    â”‚              â”‚    â”‚                  â”‚
â”‚ - Workers    â”‚    â”‚ - Embeddings â”‚    â”‚ - LLM 14B        â”‚
â”‚ - Sessions   â”‚    â”‚ - Semantic   â”‚    â”‚ - Reasoning      â”‚
â”‚ - Tracking   â”‚    â”‚   Search     â”‚    â”‚ - Thai/English   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### RAG Pipeline

```
User Query (Thai/English)
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Embedding Generator â”‚ (sentence-transformers)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚ (768-dim vector)
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Qdrant Search       â”‚ (cosine similarity)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚ (top-k results)
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prompt Builder      â”‚ (context + query)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DeepSeek-R1 (LLM)   â”‚ (generate response)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
Response (with reasoning)
```

---

## ğŸš€ Quick Start

### 1. Prerequisites

```bash
# Docker and Docker Compose installed
docker --version
docker compose version
```

### 2. Download DeepSeek-R1 Model

```bash
# Start Ollama service first
docker compose -f docker-compose.cpu.yml up ollama -d

# Pull the model (this will take some time, ~8GB download)
docker exec -it assembly_ollama ollama pull deepseek-r1:14b

# Verify model is available
docker exec -it assembly_ollama ollama list
```

### 3. Start All Services

```bash
# Start all services
docker compose -f docker-compose.cpu.yml up -d

# Check logs
docker compose -f docker-compose.cpu.yml logs -f app

# Wait for "Phase 4B RAG + DeepSeek-R1 system started successfully!"
```

### 4. Verify Services

```bash
# Check health endpoint
curl http://localhost:8000/health | jq

# Expected output:
{
  "status": "healthy",
  "phase": "Phase 4B - RAG + DeepSeek-R1",
  "components": {
    "api": "healthy",
    "postgresql": "connected",
    "qdrant": "connected",
    "redis": "available",
    "ollama": "connected"
  },
  "ai_services": {
    "knowledge_base": "ready",
    "insight_generator": "ready",
    "anomaly_detector": "ready",
    "recommendation_engine": "ready",
    "report_generator": "ready"
  }
}
```

---

## ğŸ“¡ API Endpoints

### AI Query API (`/api/v1/ai`)

#### 1. Natural Language Query

**POST** `/api/v1/ai/query`

Ask questions in natural language (Thai or English).

**Request:**
```json
{
  "question": "à¸à¸™à¸±à¸à¸‡à¸²à¸™ W001 à¸—à¸³à¸‡à¸²à¸™à¸­à¸¢à¹ˆà¸²à¸‡à¹„à¸£à¸šà¹‰à¸²à¸‡à¸§à¸±à¸™à¸™à¸µà¹‰?",
  "show_reasoning": true,
  "max_context_items": 5
}
```

**Response:**
```json
{
  "question": "à¸à¸™à¸±à¸à¸‡à¸²à¸™ W001 à¸—à¸³à¸‡à¸²à¸™à¸­à¸¢à¹ˆà¸²à¸‡à¹„à¸£à¸šà¹‰à¸²à¸‡à¸§à¸±à¸™à¸™à¸µà¹‰?",
  "answer": "à¸à¸™à¸±à¸à¸‡à¸²à¸™ W001 (à¸™à¸²à¸¢à¸ªà¸¡à¸Šà¸²à¸¢ à¹ƒà¸ˆà¸”à¸µ) à¸—à¸³à¸‡à¸²à¸™à¹„à¸”à¹‰à¸”à¸µà¸¡à¸²à¸...",
  "reasoning": "<think>à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸ knowledge base...</think>",
  "context_used": {
    "workers": 1,
    "productivity": 3,
    "sessions": 2
  },
  "model": "deepseek-r1:14b",
  "duration_ms": 1250
}
```

**Example Queries:**
- Thai: `"à¸à¸™à¸±à¸à¸‡à¸²à¸™à¸„à¸™à¹„à¸«à¸™à¸¡à¸µ productivity à¸ªà¸¹à¸‡à¸ªà¸¸à¸”à¹ƒà¸™à¸à¸°à¹€à¸Šà¹‰à¸²?"`
- English: `"Who has the highest productivity in the morning shift?"`
- Thai: `"à¹à¸™à¸°à¸™à¸³à¸à¸²à¸£à¸›à¸£à¸±à¸šà¸›à¸£à¸¸à¸‡à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸™à¸±à¸à¸‡à¸²à¸™à¸—à¸µà¹ˆà¸¡à¸µ efficiency à¸•à¹ˆà¸³à¸à¸§à¹ˆà¸² 70%"`
- English: `"Recommend improvements for workers with efficiency below 70%"`

#### 2. Worker Analysis

**POST** `/api/v1/ai/analyze/worker`

Analyze a specific worker's performance.

**Request:**
```json
{
  "worker_id": "W001",
  "include_recommendations": true
}
```

**Response:**
```json
{
  "worker_id": "W001",
  "worker_name": "à¸™à¸²à¸¢à¸ªà¸¡à¸Šà¸²à¸¢ à¹ƒà¸ˆà¸”à¸µ",
  "analysis": "à¸à¸²à¸£à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸œà¸¥à¸à¸²à¸£à¸—à¸³à¸‡à¸²à¸™...",
  "reasoning": "<think>...</think>",
  "productivity_data": {
    "index_11_overall_productivity": 85.5,
    "index_5_work_efficiency": 78.2,
    "index_9_output_per_hour": 12.3,
    "index_10_quality_score": 92.0
  },
  "model": "deepseek-r1:14b"
}
```

#### 3. Compare Workers

**POST** `/api/v1/ai/compare/workers`

Compare multiple workers' performance.

**Request:**
```json
{
  "worker_ids": ["W001", "W002", "W003"],
  "criteria": "overall_productivity"
}
```

**Response:**
```json
{
  "workers_compared": 3,
  "comparison": "à¸à¸²à¸£à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸šà¸à¸™à¸±à¸à¸‡à¸²à¸™ 3 à¸„à¸™...",
  "reasoning": "<think>...</think>",
  "workers_data": [...],
  "model": "deepseek-r1:14b"
}
```

#### 4. Shift Summary

**POST** `/api/v1/ai/summary/shift`

Generate shift performance summary.

**Request:**
```json
{
  "shift": "morning",
  "date": "2025-12-14"
}
```

**Response:**
```json
{
  "shift": "morning",
  "date": "2025-12-14",
  "summary": "à¸à¸°à¹€à¸Šà¹‰à¸² à¸§à¸±à¸™à¸—à¸µà¹ˆ 14 à¸˜à¸±à¸™à¸§à¸²à¸„à¸¡ 2025...",
  "reasoning": "<think>...</think>",
  "statistics": {
    "total_workers": 15,
    "avg_productivity": 78.5,
    "total_output": 1250,
    "issues_count": 2
  },
  "model": "deepseek-r1:14b"
}
```

#### 5. Health Check

**GET** `/api/v1/ai/health`

Check AI services health.

**Response:**
```json
{
  "ollama_client": {
    "initialized": true,
    "connected": true,
    "stats": {
      "total_requests": 42,
      "total_errors": 0,
      "avg_response_time_ms": 1150
    }
  },
  "knowledge_base": {
    "initialized": true,
    "stats": {
      "total_indexed": 250,
      "collections": 4
    }
  }
}
```

#### 6. List Models

**GET** `/api/v1/ai/models`

List available LLM models.

**Response:**
```json
{
  "models": [
    {
      "name": "deepseek-r1:14b",
      "size": "8.5GB",
      "modified": "2025-12-14T10:30:00Z"
    }
  ],
  "current_model": "deepseek-r1:14b"
}
```

---

## ğŸ§ª Testing Workflow

### Step 1: Register a Worker

```bash
curl -X POST http://localhost:8000/api/v1/workers \
  -H "Content-Type: application/json" \
  -d '{
    "worker_id": "W001",
    "name": "à¸™à¸²à¸¢à¸ªà¸¡à¸Šà¸²à¸¢ à¹ƒà¸ˆà¸”à¸µ",
    "shift": "morning",
    "role": "assembler",
    "department": "assembly_line_1"
  }'
```

### Step 2: Index Worker in Knowledge Base

The worker is automatically indexed when registered. Verify:

```bash
curl -X POST http://localhost:8000/api/v1/ai/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "à¸¡à¸µà¸à¸™à¸±à¸à¸‡à¸²à¸™à¸Šà¸·à¹ˆà¸­à¸ªà¸¡à¸Šà¸²à¸¢à¹„à¸«à¸¡?",
    "show_reasoning": false
  }'
```

### Step 3: Simulate Productivity Data

```bash
# Start a work session
curl -X POST http://localhost:8000/api/v1/workers/W001/sessions/start

# Complete some tasks (simulate work)
curl -X POST http://localhost:8000/api/v1/workers/W001/tasks \
  -H "Content-Type: application/json" \
  -d '{"task_type": "assembly", "quantity": 10}'

# End session
curl -X POST http://localhost:8000/api/v1/workers/W001/sessions/end
```

### Step 4: Query Worker Performance

```bash
curl -X POST http://localhost:8000/api/v1/ai/analyze/worker \
  -H "Content-Type: application/json" \
  -d '{
    "worker_id": "W001",
    "include_recommendations": true
  }' | jq
```

### Step 5: Generate Daily Report

```bash
curl -X POST http://localhost:8000/api/v1/ai/summary/shift \
  -H "Content-Type: application/json" \
  -d '{
    "shift": "morning",
    "date": "2025-12-14"
  }' | jq
```

### Step 6: Natural Language Query

```bash
# Thai query
curl -X POST http://localhost:8000/api/v1/ai/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "à¸à¸™à¸±à¸à¸‡à¸²à¸™à¸„à¸™à¹„à¸«à¸™à¸—à¸³à¸‡à¸²à¸™à¹„à¸”à¹‰à¸”à¸µà¸—à¸µà¹ˆà¸ªà¸¸à¸”à¸§à¸±à¸™à¸™à¸µà¹‰?",
    "show_reasoning": true
  }' | jq

# English query
curl -X POST http://localhost:8000/api/v1/ai/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Who are the top 3 performers today?",
    "show_reasoning": false
  }' | jq
```

---

## ğŸ“Š Knowledge Base Collections

### 1. Workers Collection

**Purpose:** Store worker profiles and metadata

**Indexed Fields:**
- Worker ID, name, shift, role, department
- Registration date, status
- Total sessions, total work hours

**Example Search:**
```python
results = knowledge_base.search_workers(
    query="morning shift assemblers",
    limit=10
)
```

### 2. Productivity Collection

**Purpose:** Store productivity indices and performance data

**Indexed Fields:**
- All 11 productivity indices
- Worker ID, shift, date
- Overall productivity score
- Quality score, output per hour

**Example Search:**
```python
results = knowledge_base.search_productivity(
    query="high productivity workers",
    limit=5,
    score_threshold=0.7
)
```

### 3. Sessions Collection

**Purpose:** Store work session details

**Indexed Fields:**
- Session ID, worker ID
- Start time, end time, duration
- Zone transitions, tasks completed
- Break time, idle time

**Example Search:**
```python
results = knowledge_base.search_sessions(
    query="long work sessions",
    limit=10
)
```

### 4. Insights Collection

**Purpose:** Store generated insights and summaries

**Indexed Fields:**
- Insight type (daily_summary, worker_analysis, etc.)
- Generated content
- Metadata (date, workers analyzed, etc.)

**Example Search:**
```python
results = knowledge_base.search_insights(
    query="low productivity alerts",
    limit=5
)
```

---

## ğŸ” Anomaly Detection

### Statistical Methods

**1. Productivity Anomalies**
- Z-score calculation: `(current - mean) / std`
- Threshold: 2.0 standard deviations
- Types: `unusually_high`, `unusually_low`

**2. Efficiency Drop Detection**
- Compare current vs recent average
- Default threshold: 15% drop
- Severity: `critical` (>25%), `warning` (>15%)

**3. Quality Issues**
- Track recent quality scores
- Threshold: 80/100
- Alert if >50% of scores below threshold

**4. Idle Time Spikes**
- Compare to historical average
- Spike multiplier: 2.0x
- Severity: `warning`

**5. Output Decline**
- Compare current output vs average
- Threshold: 20% decline
- Severity: `critical` (>35%), `warning` (>20%)

### Example Usage

```python
from insights.anomaly_detector import AnomalyDetector

detector = AnomalyDetector(std_threshold=2.0)

result = detector.detect_productivity_anomalies(
    current_value=55.0,
    historical_values=[75, 78, 80, 77, 76]
)

# result:
{
  "is_anomaly": True,
  "anomaly_type": "unusually_low",
  "z_score": -2.5,
  "severity": "critical",
  "deviation_percent": -27.6
}
```

---

## ğŸ¯ Recommendation Engine

### Priority Levels

- **High:** Critical issues requiring immediate action
- **Medium:** Important improvements for better performance
- **Low:** Optional optimizations
- **Info:** Informational feedback

### Recommendation Categories

1. **Productivity Improvement**
   - Low overall productivity (<60)
   - Declining productivity trend

2. **Efficiency Enhancement**
   - Low work efficiency (<70%)
   - Excessive idle time (>2 hours)

3. **Quality Control**
   - Quality score below 80
   - Consistent quality issues

4. **Output Optimization**
   - Output below target
   - Declining output trend

5. **Time Management**
   - Excessive break time
   - Poor time utilization

### Example Output

```json
{
  "recommendations": [
    {
      "priority": "high",
      "category": "productivity",
      "message": "Overall productivity is 55/100 (below target of 60). Immediate action needed.",
      "actionable_steps": [
        "Review work processes",
        "Provide additional training",
        "Check for equipment issues"
      ]
    },
    {
      "priority": "medium",
      "category": "efficiency",
      "message": "Work efficiency is 68% (target: 70%). Consider workflow optimization.",
      "actionable_steps": [
        "Analyze workflow bottlenecks",
        "Optimize task sequencing"
      ]
    }
  ]
}
```

---

## ğŸ“„ Report Generation

### Daily Report

```python
from reports.report_generator import ReportGenerator

report = await report_generator.generate_daily_report(
    date=datetime(2025, 12, 14),
    language="th"  # or "en"
)

print(report_generator.format_daily_report(report))
```

**Output:**
```
=== à¸£à¸²à¸¢à¸‡à¸²à¸™à¸›à¸£à¸°à¸ˆà¸³à¸§à¸±à¸™: 14 à¸˜à¸±à¸™à¸§à¸²à¸„à¸¡ 2025 ===

ğŸ“Š à¸ªà¸£à¸¸à¸›à¸ à¸²à¸à¸£à¸§à¸¡:
  â€¢ à¸à¸™à¸±à¸à¸‡à¸²à¸™à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”: 45 à¸„à¸™
  â€¢ à¸œà¸¥à¸œà¸¥à¸´à¸•à¹€à¸‰à¸¥à¸µà¹ˆà¸¢: 76.5/100
  â€¢ à¸œà¸¥à¸œà¸¥à¸´à¸•à¸£à¸§à¸¡: 2,450 à¸Šà¸´à¹‰à¸™
  â€¢ à¸¡à¸µà¸›à¸±à¸à¸«à¸²: 3 à¸„à¸™

ğŸŒŸ à¸à¸™à¸±à¸à¸‡à¸²à¸™à¸¢à¸­à¸”à¹€à¸¢à¸µà¹ˆà¸¢à¸¡ (Top 5):
  1. à¸™à¸²à¸¢à¸ªà¸¡à¸Šà¸²à¸¢ à¹ƒà¸ˆà¸”à¸µ (W001) - 95.2/100
  2. à¸™à¸²à¸‡à¸ªà¸²à¸§à¸¡à¸²à¸¥à¸µ à¸£à¸±à¸à¸‡à¸²à¸™ (W015) - 92.8/100
  ...

âš ï¸ à¸à¸™à¸±à¸à¸‡à¸²à¸™à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸±à¸’à¸™à¸²:
  â€¢ à¸™à¸²à¸¢à¸šà¸¸à¸à¸¡à¸µ à¸‚à¸¢à¸±à¸™ (W042) - 52.3/100
    â†’ à¸›à¸±à¸à¸«à¸²: efficiency à¸•à¹ˆà¸³, idle time à¸ªà¸¹à¸‡

ğŸ“ˆ à¸à¸²à¸£à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸•à¸²à¸¡à¸à¸°:
  à¸à¸°à¹€à¸Šà¹‰à¸²: à¸œà¸¥à¸œà¸¥à¸´à¸•à¹€à¸‰à¸¥à¸µà¹ˆà¸¢ 78.2/100 (à¸”à¸µ)
  à¸à¸°à¸šà¹ˆà¸²à¸¢: à¸œà¸¥à¸œà¸¥à¸´à¸•à¹€à¸‰à¸¥à¸µà¹ˆà¸¢ 75.8/100 (à¸à¸­à¹ƒà¸Šà¹‰)
  à¸à¸°à¸”à¸¶à¸: à¸œà¸¥à¸œà¸¥à¸´à¸•à¹€à¸‰à¸¥à¸µà¹ˆà¸¢ 74.1/100 (à¸à¸­à¹ƒà¸Šà¹‰)

ğŸ¤– AI Analysis:
à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¹‚à¸”à¸¢ DeepSeek-R1...
[AI-generated summary]

ğŸ“‹ à¸„à¸³à¹à¸™à¸°à¸™à¸³:
  1. à¸à¸™à¸±à¸à¸‡à¸²à¸™ 3 à¸„à¸™à¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸à¸²à¸£à¸à¸±à¸’à¸™à¸²
  2. à¸à¸°à¸”à¸¶à¸à¸¡à¸µà¸œà¸¥à¸œà¸¥à¸´à¸•à¸•à¹ˆà¸³à¸à¸§à¹ˆà¸²à¹€à¸›à¹‰à¸²à¸«à¸¡à¸²à¸¢
  ...
```

### Weekly Report

```python
report = await report_generator.generate_weekly_report(
    start_date=datetime(2025, 12, 8)
)

print(report_generator.format_weekly_report(report))
```

### Worker Report

```python
report = await report_generator.generate_worker_report(
    worker_id="W001",
    days=30
)

print(report_generator.format_worker_report(report))
```

---

## ğŸ› ï¸ Configuration

### Environment Variables

```env
# Ollama
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_MODEL=deepseek-r1:14b

# Qdrant
QDRANT_HOST=qdrant
QDRANT_PORT=6333

# Embedding Model
EMBEDDING_MODEL=sentence-transformers/paraphrase-multilingual-mpnet-base-v2
EMBEDDING_DIM=768

# Thresholds
MIN_PRODUCTIVITY_THRESHOLD=60.0
MIN_EFFICIENCY_THRESHOLD=70.0
ANOMALY_STD_THRESHOLD=2.0
```

### Customizing Thresholds

Edit `src/main.py`:

```python
# Adjust insight generator thresholds
insight_generator = InsightGenerator(
    ollama_client=ollama_client,
    knowledge_base=knowledge_base,
    min_productivity_threshold=65.0,  # Increase from 60
    min_efficiency_threshold=75.0     # Increase from 70
)

# Adjust anomaly detector sensitivity
anomaly_detector = AnomalyDetector(
    std_threshold=1.5,     # More sensitive (default: 2.0)
    min_data_points=10     # Require more data (default: 5)
)
```

---

## ğŸ› Troubleshooting

### 1. Ollama Connection Failed

**Symptom:** `Connection refused to ollama:11434`

**Solution:**
```bash
# Check if Ollama is running
docker compose ps ollama

# Restart Ollama
docker compose restart ollama

# Check logs
docker compose logs ollama
```

### 2. Model Not Found

**Symptom:** `Model 'deepseek-r1:14b' not found`

**Solution:**
```bash
# Pull the model
docker exec -it assembly_ollama ollama pull deepseek-r1:14b

# Verify
docker exec -it assembly_ollama ollama list
```

### 3. Slow Response Times

**Symptom:** AI queries take >10 seconds

**Solutions:**
- **CPU Mode:** DeepSeek-R1:14b is slow on CPU. Consider using GPU version.
- **Smaller Model:** Use `deepseek-r1:7b` or `llama3:8b` for faster responses.
- **Disable Reasoning:** Set `show_reasoning=false` in queries.

```bash
# Pull smaller model
docker exec -it assembly_ollama ollama pull llama3:8b

# Update environment variable
OLLAMA_MODEL=llama3:8b
```

### 4. Qdrant Connection Issues

**Symptom:** `Failed to connect to Qdrant`

**Solution:**
```bash
# Check Qdrant status
docker compose ps qdrant

# Restart Qdrant
docker compose restart qdrant

# Verify collections
curl http://localhost:6333/collections
```

### 5. Out of Memory

**Symptom:** `Ollama killed (OOM)`

**Solution:**
```yaml
# Edit docker-compose.cpu.yml
ollama:
  deploy:
    resources:
      limits:
        memory: 16G  # Increase from default
```

---

## ğŸ“ˆ Performance Optimization

### 1. Enable GPU Acceleration

```bash
# Use GPU docker-compose file (if available)
docker compose -f docker-compose.yml up -d

# Verify GPU usage
docker exec -it assembly_ollama nvidia-smi
```

### 2. Batch Processing

```python
# Process multiple workers in parallel
async def analyze_multiple_workers(worker_ids):
    tasks = [
        ai_query.analyze_worker(worker_id)
        for worker_id in worker_ids
    ]
    return await asyncio.gather(*tasks)
```

### 3. Cache Results

```python
from functools import lru_cache
import hashlib

@lru_cache(maxsize=100)
def get_cached_analysis(worker_id: str, date: str):
    # Cache worker analysis for the day
    pass
```

### 4. Optimize Embedding Generation

```python
# Batch encode texts
texts = [worker1_text, worker2_text, ...]
embeddings = embedding_generator.encode(texts)  # Single batch call
```

---

## ğŸ” Security Considerations

### 1. API Authentication

Add authentication middleware:

```python
from fastapi import Security, HTTPException
from fastapi.security import HTTPBearer

security = HTTPBearer()

@app.middleware("http")
async def verify_token(request: Request, call_next):
    # Verify JWT token
    # ...
    response = await call_next(request)
    return response
```

### 2. Rate Limiting

```python
from slowapi import Limiter
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter

@app.post("/api/v1/ai/query")
@limiter.limit("10/minute")  # Max 10 queries per minute
async def natural_language_query(...):
    ...
```

### 3. Input Validation

```python
from pydantic import BaseModel, Field, validator

class QueryRequest(BaseModel):
    question: str = Field(..., min_length=1, max_length=500)

    @validator('question')
    def sanitize_input(cls, v):
        # Prevent injection attacks
        return v.strip()
```

---

## ğŸ“š Additional Resources

- **DeepSeek-R1 Documentation:** https://ollama.com/library/deepseek-r1
- **Qdrant Documentation:** https://qdrant.tech/documentation/
- **Sentence Transformers:** https://www.sbert.net/
- **FastAPI Documentation:** https://fastapi.tiangolo.com/

---

## ğŸ“ Example Use Cases

### Use Case 1: Daily Standup Report

**Goal:** Generate morning report for management

```bash
curl -X POST http://localhost:8000/api/v1/ai/summary/shift \
  -H "Content-Type: application/json" \
  -d '{"shift": "morning", "date": "2025-12-14"}' \
  | jq '.summary'
```

### Use Case 2: Worker Performance Review

**Goal:** Review worker performance for monthly evaluation

```bash
curl -X POST http://localhost:8000/api/v1/ai/analyze/worker \
  -H "Content-Type: application/json" \
  -d '{"worker_id": "W001", "include_recommendations": true}' \
  | jq
```

### Use Case 3: Identify Training Needs

**Goal:** Find workers who need additional training

```bash
curl -X POST http://localhost:8000/api/v1/ai/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "à¹à¸ªà¸”à¸‡à¸£à¸²à¸¢à¸Šà¸·à¹ˆà¸­à¸à¸™à¸±à¸à¸‡à¸²à¸™à¸—à¸µà¹ˆà¸¡à¸µ quality score à¸•à¹ˆà¸³à¸à¸§à¹ˆà¸² 70 à¹à¸¥à¸°à¹à¸™à¸°à¸™à¸³à¸«à¸¥à¸±à¸à¸ªà¸¹à¸•à¸£à¸—à¸µà¹ˆà¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡"
  }' | jq '.answer'
```

### Use Case 4: Compare Shift Performance

**Goal:** Compare productivity across shifts

```bash
curl -X POST http://localhost:8000/api/v1/ai/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸šà¸œà¸¥à¸œà¸¥à¸´à¸•à¸‚à¸­à¸‡à¸à¸°à¹€à¸Šà¹‰à¸² à¸à¸°à¸šà¹ˆà¸²à¸¢ à¹à¸¥à¸°à¸à¸°à¸”à¸¶à¸ à¹à¸¥à¹‰à¸§à¹à¸™à¸°à¸™à¸³à¸›à¸£à¸±à¸šà¸›à¸£à¸¸à¸‡"
  }' | jq '.answer'
```

---

## ğŸ“ Changelog

### Version 4.1.0 (Phase 4B) - 2025-12-14

**Added:**
- âœ¨ RAG system with Qdrant vector database
- âœ¨ DeepSeek-R1 integration via Ollama
- âœ¨ Natural language query API (Thai/English)
- âœ¨ Automated insight generation
- âœ¨ Statistical anomaly detection
- âœ¨ AI-powered recommendation engine
- âœ¨ Automated report generation
- âœ¨ Knowledge base with 4 collections
- âœ¨ Bilingual prompt templates
- âœ¨ Comprehensive health check endpoint

**Changed:**
- ğŸ”„ Updated main.py to initialize AI services
- ğŸ”„ Enhanced health check with AI service status
- ğŸ”„ Version bump: 4.0.0 â†’ 4.1.0

**Technical:**
- ğŸ“¦ LLM: DeepSeek-R1:14b (8.5GB)
- ğŸ“¦ Embeddings: paraphrase-multilingual-mpnet-base-v2 (768-dim)
- ğŸ“¦ Vector DB: Qdrant (cosine similarity)
- ğŸ“¦ Framework: FastAPI + AsyncIO

---

## ğŸ¤ Contributing

Phase 4B is complete. Future enhancements:

- [ ] Add caching layer (Redis)
- [ ] Implement user authentication
- [ ] Add Grafana dashboards
- [ ] Support more LLM models
- [ ] Add voice interface
- [ ] Mobile app integration

---

## ğŸ“„ License

Assembly Time-Tracking System - Internal Use Only

---

**Phase 4B Complete** âœ…
**Next:** Phase 4C - Advanced Analytics (Optional)
